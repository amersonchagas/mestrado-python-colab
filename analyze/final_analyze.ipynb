{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe3IoZ338PDk"
      },
      "source": [
        "#Libs and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB0yRPKCbWya"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpSZclcA7rUy"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=126dO4VNhLpYKT0TKp18RTAGjrHAl_ZpU -O mnist.npz\n",
        "!gdown https://drive.google.com/uc?id=16wlkaf6GCGX0aJTOtzDo0ypnhqYZ7GVM -O kmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=1XMH39lcD2bnwy4AW3S-4-0Ge7JIYw6CF -O eurosat.npz\n",
        "!gdown https://drive.google.com/uc?id=1BsfU84WJMRRKG3wzRZG6KuCperlLrxHc -O cifar10.npz\n",
        "!gdown https://drive.google.com/uc?id=1MvPjY4m58TW51NZbUIl5tRJbERKMnIk7 -O pathmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=146WDl2VzVdLhnJl5JqYqKVyDPlDQoLDl -O octmnist.npz\n",
        "!gdown https://drive.google.com/uc?id=1BIJFOn5ivB766qNIZdI2Owt8GAmpsmic -O organmnist_axial.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHwZmGUNbTRf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import callbacks, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50V2, EfficientNetB1, DenseNet169\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Lambda, Input\n",
        "from tensorflow.image import resize\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Orange.evaluation import compute_CD, graph_ranks\n",
        "from scipy.stats import friedmanchisquare, rankdata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re, os, time, requests\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhvasHsa8VPv"
      },
      "source": [
        "#Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXE-7P_oKP3G"
      },
      "outputs": [],
      "source": [
        "DATASETS = {\n",
        "    'cifar10': {\n",
        "        'shape': (32, 32, 3),\n",
        "        'classes': 10,\n",
        "        'phenotypes': ['(((conv*3)bnorm-pool-dropout)*3)fc*1*256*lr-0.001'],\n",
        "    },\n",
        "    'mnist': {\n",
        "        'shape': (28, 28, 1),\n",
        "        'classes': 10,\n",
        "        'phenotypes': ['(((conv*3)bnorm-pool-dropout)*3)fc*1*256*lr-0.001'],\n",
        "    },\n",
        "    'eurosat': {\n",
        "        'shape': (64, 64, 3),\n",
        "        'classes': 10,\n",
        "        'phenotypes': ['(((conv*3)bnorm-pool-dropout)*3)fc*1*256*lr-0.001'],\n",
        "    },\n",
        "}\n",
        "NUM_TRAINING = 3\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKmIcKBj8eO-"
      },
      "source": [
        "#Factories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKOm_msFbalE"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name):\n",
        "\n",
        "  shape = DATASETS[dataset_name]['shape']\n",
        "  dataset = np.load('%s.npz' % dataset_name, allow_pickle=True)\n",
        "\n",
        "  if dataset_name == 'eurosat':\n",
        "      \n",
        "    print('eurosat')\n",
        "    \n",
        "    train = dataset['train'].tolist()\n",
        "\n",
        "    train_images, train_labels = train['image'], train['label']\n",
        "\n",
        "    train_images = train_images.reshape((train_images.shape[0], *shape))\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "    validation_images, test_images, validation_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  elif dataset_name in ['pathmnist', 'octmnist', 'organmnist_axial']:\n",
        "      \n",
        "    print('medmnist:', dataset_name)\n",
        "    \n",
        "    train_images = dataset['train_images']\n",
        "    validation_images = dataset['val_images']\n",
        "    test_images = dataset['test_images']\n",
        "    train_labels = dataset['train_labels']\n",
        "    validation_labels = dataset['val_labels']\n",
        "    test_labels = dataset['test_labels']\n",
        "\n",
        "    if shape[2] == 1:\n",
        "      train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "      validation_images = validation_images.reshape((validation_images.shape[0], 28, 28, 1))\n",
        "      test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "    test_images = test_images.astype(\"float\") / 255.0\n",
        "    validation_images = validation_images.astype(\"float\") / 255.0\n",
        "\n",
        "  else:\n",
        "      \n",
        "    print('outros:', dataset_name)\n",
        "    \n",
        "    train = dataset['train'].tolist()\n",
        "    test = dataset['test'].tolist()\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train['image'], test['image'], train['label'], test['label']\n",
        "\n",
        "    train_images = train_images.reshape((train_images.shape[0], *shape))\n",
        "    train_images = train_images.astype(\"float\") / 255.0\n",
        "\n",
        "    test_images = test_images.reshape((test_images.shape[0], *shape))\n",
        "    test_images = test_images.astype(\"float\") / 255.0\n",
        "\n",
        "    validation_images, test_images, validation_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  lb = LabelBinarizer()\n",
        "  train_labels = lb.fit_transform(train_labels)\n",
        "  validation_labels = lb.transform(validation_labels)\n",
        "  test_labels = lb.transform(test_labels)\n",
        "\n",
        "  dataset.close()\n",
        "\n",
        "  return train_images, train_labels, validation_images, validation_labels, test_images, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41QD3TOGjZiC"
      },
      "outputs": [],
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "  \n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = true_positives / (predicted_positives + K.epsilon())\n",
        "  recall = true_positives / (possible_positives + K.epsilon())\n",
        "  f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "  return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQEKzjs78zaA"
      },
      "outputs": [],
      "source": [
        "def build_model(dataset, phenotype):\n",
        "\n",
        "    dataset_shape = DATASETS[dataset]['shape']\n",
        "    dataset_classes = DATASETS[dataset]['classes']\n",
        "\n",
        "    nconv, npool, nfc, nfcneuron = [int(i) for i in re.findall('\\d+', phenotype.split('lr-')[0])]\n",
        "    has_dropout = 'dropout' in phenotype\n",
        "    has_batch_normalization = 'bnorm' in phenotype\n",
        "    has_pool = 'pool' in phenotype\n",
        "    learning_rate = float(phenotype.split('lr-')[1])\n",
        "\n",
        "    # number of filters\n",
        "    filter_size = 32\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=dataset_shape))\n",
        "\n",
        "    # Pooling\n",
        "    for i in range(npool):\n",
        "\n",
        "        # Convolutions\n",
        "        for j in range(nconv):\n",
        "\n",
        "            model.add(layers.Conv2D(filter_size, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "            # Duplicate number of filters for each two convolutions\n",
        "            if (((i + j) % 2) == 1): filter_size = filter_size * 2\n",
        "\n",
        "            # Add batch normalization\n",
        "            if has_batch_normalization:\n",
        "                model.add(layers.BatchNormalization())\n",
        "\n",
        "        # Add pooling\n",
        "        if has_pool:\n",
        "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "            # Add dropout\n",
        "            if has_dropout:\n",
        "                model.add(layers.Dropout(0.25))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # fully connected\n",
        "    for i in range(nfc):\n",
        "        model.add(layers.Dense(nfcneuron))\n",
        "        model.add(layers.Activation('relu'))\n",
        "\n",
        "    if has_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "\n",
        "    model.add(layers.Dense(dataset_classes, activation='softmax'))\n",
        "\n",
        "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    return model, opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-KjZKEH8j3a"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6eGNXPReUoX"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset):\n",
        "\n",
        "  train_images, train_labels, validation_images, \\\n",
        "    validation_labels, test_images, test_labels = load_dataset(dataset)\n",
        "\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "  validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  accuracies, f1_scores = [], []\n",
        "\n",
        "  for i in range(NUM_TRAINING):\n",
        "\n",
        "    print('Training %s of %s' % (i + 1, NUM_TRAINING))\n",
        "\n",
        "    history = model.fit(train_ds,\n",
        "            epochs=EPOCHS, \n",
        "            validation_data=validation_ds,\n",
        "            verbose=1)\n",
        "\n",
        "    loss, accuracy, f1_score = model.evaluate(test_images, test_labels, verbose=1)\n",
        "\n",
        "    print(accuracy, f1_score)\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    f1_scores.append(f1_score)\n",
        "\n",
        "  return {\n",
        "      'accuracy': np.mean(accuracies),\n",
        "      'accuracy_sd': np.std(accuracies),\n",
        "      'f1_score': np.mean(f1_scores),\n",
        "      'f1_score_sd': np.std(f1_scores),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnEp86OdlYRX"
      },
      "outputs": [],
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JwD-VuR3KuI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for dataset in DATASETS:\n",
        "\n",
        "  for phenotype in DATASETS[dataset]['phenotypes']:\n",
        "\n",
        "    print('DATASET:', dataset)\n",
        "    print('PHENOTYPE:', phenotype)\n",
        "\n",
        "    print('Building model.')\n",
        "\n",
        "    model, opt = build_model(dataset, phenotype)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', f1_score])\n",
        "    # model.summary()\n",
        "\n",
        "    print('Model created.')\n",
        "\n",
        "    print('Begining training...')\n",
        "\n",
        "    fitness = train_model(model, dataset)\n",
        "\n",
        "    print('FITNESS:', fitness)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "analise_amerson.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}